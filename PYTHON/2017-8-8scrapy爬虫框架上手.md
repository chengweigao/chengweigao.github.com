今天来装一下python的爬虫框架

但是对于我的centos7+python2.7.5，报了下面的错误

     src/twisted/test/raiser.c:4:20: 致命错误：Python.h：没有那个文件或目录
     #include "Python.h"编译中断。
    error: command 'gcc' failed with exit status 1
    
    ----------------------------------------
    Command "/usr/bin/python2 -u -c "import setuptools, tokenize;
	__file__='/tmp/pip-build-hXIdgq/twisted/setup.py';
	f=getattr(tokenize, 'open', open)(__file__);
	code=f.read().replace('\r\n', '\n');
	f.close();
	exec(compile(code, __file__, 'exec'))" install --record /tmp/pip-pJOp2_-record/install-record.txt --single-version-externally-managed --compile" failed with error code 1 in /tmp/pip-build-hXIdgq/twisted/

于是需要先安装python-devel

sudo yum install python-devel　　

#注意这里不是python-dev

安装pip 

 yum install python-pip

再安装 Twisted 模块限制版本所以必须使用==

 pip install Twisted==13.1.0

安装Scrapy

 pip install Scrapy

开始创建爬虫项目

	scrapy startproject tutorial
	这将创建一个tutorial以下内容目录：
	
	tutorial/
    scrapy.cfg            # deploy configuration file

    tutorial/             # project's Python module, you'll import your code from here
        __init__.py

        items.py          # project items definition file

        pipelines.py      # project pipelines file

        settings.py       # project settings file

        spiders/          # a directory where you'll later put your spiders
            __init__.py

将如下demo创建保存到tutorial/spiders目录下  注意缩进

	import scrapy
	class QuotesSpider(scrapy.Spider):
	    name = "quotes"
	
	    def start_requests(self):
	        urls = [
	            'http://quotes.toscrape.com/page/1/',
	            'http://quotes.toscrape.com/page/2/',
	        ]
	        for url in urls:
	            yield scrapy.Request(url=url, callback=self.parse)
	
	    def parse(self, response):
	        page = response.url.split("/")[-2]
	        filename = 'quotes-%s.html' % page
	        with open(filename, 'wb') as f:
	            f.write(response.body)
	        self.log('Saved file %s' % filename)

把我们的蜘蛛的工作 cd tutorial/  ，去项目的顶层目录并运行：

    scrapy crawl quotes
该命令将蜘蛛的名称quotes我们只是说，能把一些要求quotes.toscrape.com域你会得到一个outputsimilar这：


	(omitted for brevity)
	2016-12-16 21:24:05 [scrapy.core.engine] INFO: Spider opened
	2016-12-16 21:24:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
	2016-12-16 21:24:05 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
	2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://quotes.toscrape.com/robots.txt> (referer: None)
	2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/1/> (referer: None)
	2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/2/> (referer: None)
	2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-1.html
	2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-2.html
	2016-12-16 21:24:05 [scrapy.core.engine] INFO: Closing spider (finished)

现在，检查当前目录中的文件。你应该注意到两newfiles已创建：quotes-1.html和quotes-2.html内容，与各自的URL，我们parse方法指示

文章参照http://scrapy-chs.readthedocs.io/zh_CN/0.24/intro/tutorial.html